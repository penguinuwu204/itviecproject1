# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4wUTEVwmD2xngPbXQcaCH0kCcOfMBAi
"""



# app.py

import streamlit as st
import pandas as pd
import numpy as np
import regex, re, unicodedata
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from deep_translator import GoogleTranslator
import spacy
import scipy.sparse as sp

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay

from gensim import corpora, models, similarities
from gensim.models import Word2Vec, FastText

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.classification import LogisticRegression as SparkLR, DecisionTreeClassifier as SparkDT, RandomForestClassifier as SparkRF

# â”€â”€â”€ Page config & Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ITViec Explorer", layout="wide")
with st.sidebar:
    st.title("ðŸ” ITViec Explorer")
    menu = st.radio("â˜° Menu", [
        "ðŸ—‚ Upload Data",
        "ðŸ“Š EDA & WordCloud",
        "ðŸ” Similarity Search",
        "ðŸ¤– Recommendation"
    ])
    st.markdown("---")
    st.markdown("### ðŸ‘¤ ThÃ nh viÃªn")
    st.markdown("- Nguyá»…n LÆ° Báº£o Khang (nbaokhang12@gmail.com)")
    st.markdown("- Pháº¡m Äá»©c Huy (DucHuyUFM@gmail.com)")
    st.markdown("**GVHD:** Ms. Khuáº¥t ThÃ¹y PhÆ°Æ¡ng")
    st.markdown("Äá»“ Ã¡n Tá»‘t nghiá»‡p  \nData Science & ML  \nTTTH - ÄH KHTN")

# â”€â”€â”€ 1) Introduction (always visible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ðŸ“ Giá»›i thiá»‡u")
st.markdown("""
**ITViec** lÃ  ná»n táº£ng viá»‡c lÃ m IT hÃ ng Ä‘áº§u Viá»‡t Nam.
- HÆ¡n 8.000 Ä‘Ã¡nh giÃ¡ nhÃ¢n viÃªn & cá»±u nhÃ¢n viÃªn.
- CÃ¡c trÆ°á»ng chÃ­nh: *What I liked*, *Suggestions for improvement*, *Company Name*, *id*, *Recommend?*, *Overall Rating*,â€¦
- Dá»¯ liá»‡u Ä‘Æ°á»£c dá»‹ch (Viâ†’En), chuáº©n hÃ³a & lá»c tá»« loáº¡i trÆ°á»›c khi dÃ¹ng ML.

**Má»¥c tiÃªu**
1. **Content-Based Similarity** (Overview & Reviews qua TF-IDF, Gensim-TFIDF, Word2Vec, FastText vÃ  numeric ratings)
2. **Recommendation Classification** (Scikit-Learn: LR, RF, SVM, XGB; PySpark: LR, DT, RF; interactive)
""")

# â”€â”€â”€ 2) Text tools & clean_text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def init_text_tools():
    tr = GoogleTranslator(source="auto", target="en")
    nlp = spacy.load("en_core_web_sm", disable=["parser","ner"])
    sw  = spacy.lang.en.stop_words.STOP_WORDS
    vn  = re.compile(r"[Ã Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ä‘Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³á»·á»¹á»±]", re.IGNORECASE)
    return tr, nlp, sw, vn

translator, nlp, EN_SW, VIET_REGEX = init_text_tools()

@st.cache_data
def clean_text(txt: str) -> str:
    s = str(txt).strip()
    if VIET_REGEX.search(s):
        try: s = translator.translate(s)
        except: pass
    s = unicodedata.normalize("NFD", s)
    s = regex.sub(r"\p{Mn}", "", s)
    s = re.sub(r"[^A-Za-z\s]", " ", s).lower()
    toks = [w for w in re.findall(r"\b\w\w+\b", s) if w not in EN_SW]
    doc  = nlp(" ".join(toks))
    return " ".join(tok.text for tok in doc if tok.pos_ in {"NOUN","VERB","ADJ","ADV"})

# â”€â”€â”€ 3) Upload Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if menu == "ðŸ—‚ Upload Data":
    st.subheader("1ï¸âƒ£ Upload dá»¯ liá»‡u")
    if "df_comp" not in st.session_state:
        src = st.selectbox("Chá»n nguá»“n:", ["A. Raw Excel (3 files)", "B. Preprocessed CSV + Reviews.xlsx"])
        if src.startswith("A"):
            comp_fp = st.file_uploader("Overview_Companies.xlsx", type="xlsx")
            map_fp  = st.file_uploader("Overview_Reviews.xlsx",  type="xlsx")
            rev_fp  = st.file_uploader("Reviews.xlsx",          type="xlsx")
            if comp_fp and map_fp and rev_fp:
                st.session_state.src = "A"
                st.session_state.comp_fp = comp_fp
                st.session_state.map_fp  = map_fp
                st.session_state.rev_fp  = rev_fp
                st.success("âœ… ÄÃ£ upload 3 file raw.")
        else:
            comp_fp = st.file_uploader("df_comp_clean.csv",       type="csv")
            all_fp  = st.file_uploader("df_all_preprocessed.csv", type="csv")
            map_fp  = st.file_uploader("Overview_Reviews.xlsx",   type="xlsx")
            if comp_fp and all_fp and map_fp:
                st.session_state.src    = "B"
                st.session_state.comp_fp = comp_fp
                st.session_state.all_fp  = all_fp
                st.session_state.map_fp  = map_fp
                st.success("âœ… ÄÃ£ upload CSV & XLSX.")
    else:
        st.info("âœ… Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng trong session.")
    st.stop()

# â”€â”€â”€ 4) Load & Preprocess â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_all(src, comp_fp, map_fp, rev_fp_or_all_fp):
    if src == "A":
        df_comp = pd.read_excel(comp_fp)
        df_map  = pd.read_excel(map_fp)
        df_rev  = pd.read_excel(rev_fp_or_all_fp)
        df_comp["Clean_desc"] = df_comp["Company overview"].fillna("").map(clean_text)
        df_rev["Clean_rev"]   = (
            df_rev["What I liked"].fillna("") + " " +
            df_rev["Suggestions for improvement"].fillna("")
        ).map(clean_text)
        df_all = (
            df_map
            .merge(df_rev, on="id", how="inner")
            .merge(df_comp[["id","Company Name","Clean_desc"]], on="id", how="inner")
        )
    else:
        # src == "B"
        df_comp = pd.read_csv(comp_fp)
        df_map  = pd.read_excel(map_fp)
        df_all  = pd.read_csv(rev_fp_or_all_fp)
        df_comp["Clean_desc"] = df_comp["Clean_desc"].fillna("")
        df_all["Clean_rev"]   = df_all["Clean_rev"].fillna("")

    # numeric ratings
    df_map_num = df_map.select_dtypes(include="number").fillna(0).copy()
    df_map_num["id"] = df_map["id"]

    # merge overview, reviews, ratings
    df_all = (
        df_all
        .merge(df_map_num, on="id", how="left")
        .merge(df_comp[["id","Company Name","Clean_desc"]], on="id", how="left")
    )
    df_all["Clean_rev"] = df_all["Clean_rev"].fillna("")
    df_all["Label"]     = (df_all["Recommend?"].str.lower() == "yes").astype(int)

    return df_comp, df_map_num, df_all

# Thay vÃ¬ load_all(st.session_state), gá»i:
src = st.session_state["src"]         # hoáº·c "A"/"B"
fp1 = st.session_state["comp_fp"]
fp2 = st.session_state["map_fp"]
fp3 = st.session_state["rev_fp"] if src=="A" else st.session_state["all_fp"]

df_comp, df_map, df_all = load_all(src, fp1, fp2, fp3)
COMP_NAMES = df_comp["Company Name"].tolist()

# â”€â”€â”€ 5) EDA & WordCloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if menu == "ðŸ“Š EDA & WordCloud":
    st.header("2ï¸âƒ£ EDA & WordCloud")
    t1,t2,t3 = st.tabs(["Companies","Numeric Ratings","Reviews"])
    with t1:
        st.subheader("Companies")
        st.dataframe(df_comp)
        txt = " ".join(df_comp["Clean_desc"])
        st.image(WordCloud(width=600,height=400).generate(txt).to_array(), use_column_width=True)
    with t2:
        st.subheader("Numeric Ratings")
        st.dataframe(df_map)
        st.bar_chart(df_map.set_index("id").mean())
    with t3:
        st.subheader("Reviews")
        st.dataframe(df_all[["Clean_rev","Recommend?","Label"]])
        txt2 = " ".join(df_all["Clean_rev"])
        st.image(WordCloud(width=600,height=400).generate(txt2).to_array(), use_column_width=True)
    st.stop()

# â”€â”€â”€ 6) Similarity Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if menu == "ðŸ” Similarity Search":
    st.header("3ï¸âƒ£ Similarity Search")
    idx = st.selectbox("Chá»n cÃ´ng ty", range(len(COMP_NAMES)), format_func=lambda i:COMP_NAMES[i])
    cid = df_comp.at[idx,"id"]

    # helper: topn table
    def show_topn(arr, labels):
        df = pd.DataFrame({"Company":labels,"Score":arr})
        return df.sort_values("Score",ascending=False).iloc[1:6].style.format({"Score":"{:.2%}"})

    # Numeric
    sim_num = cosine_similarity(df_map.drop("id", axis=1).values)[idx]
    st.subheader("â€¢ Numeric Ratings"); st.table(show_topn(sim_num,COMP_NAMES))

    # Overview TF-IDF
    tf    = TfidfVectorizer(ngram_range=(1,2),max_features=3000)
    M_tf  = tf.fit_transform(df_comp["Clean_desc"])
    st.subheader("â€¢ Overview TF-IDF"); st.table(show_topn(cosine_similarity(M_tf)[idx],COMP_NAMES))

    # Gensim-TFIDF
    toks = [t.split() for t in df_comp["Clean_desc"]]
    dct  = corpora.Dictionary(toks)
    corp_= [dct.doc2bow(t) for t in toks]
    mgt  = models.TfidfModel(corp_)
    idxs = similarities.MatrixSimilarity(mgt[corp_], num_features=len(dct))
    SIM_GS = np.vstack([ idxs[mgt[corp_][i]] for i in range(len(corp_)) ])
    st.subheader("â€¢ Overview Gensim-TFIDF"); st.table(show_topn(SIM_GS[idx],COMP_NAMES))

    # Word2Vec & FastText (IDF-weighted)
    idf = tf.idf_
    def idf_sim(model):
        mat=[]
        for doc in toks:
            vecs, ws = [], []
            for w in doc:
                if w in model.wv:
                    vecs.append(model.wv[w]); ws.append(idf[tf.vocabulary_.get(w,0)])
            mat.append(np.average(vecs,axis=0,weights=ws) if vecs else np.zeros(model.vector_size))
        return cosine_similarity(np.vstack(mat))[idx]
    st.subheader("â€¢ Overview Word2Vec"); st.table(show_topn(idf_sim(Word2Vec(toks,vector_size=100,window=5,min_count=2,epochs=10)),COMP_NAMES))
    st.subheader("â€¢ Overview FastText"); st.table(show_topn(idf_sim(FastText(toks,vector_size=100,window=5,min_count=2,epochs=10)),COMP_NAMES))

        # --- Reviews tÆ°Æ¡ng tá»± ---
    # 1) Group & reindex theo df_comp["id"]
    rev_grp = (
        df_all
        .groupby("id")["Clean_rev"]
        .apply(" ".join)
        .reindex(df_comp["id"])
        .fillna("")
    )

    # 2) Reviews TF-IDF
    tf_r    = TfidfVectorizer(ngram_range=(1,2), max_features=3000)
    M_rtf   = tf_r.fit_transform(rev_grp)
    sim_rtf = cosine_similarity(M_rtf)[idx]
    st.subheader("â€¢ Reviews TF-IDF")
    df_rtf  = pd.DataFrame({"Company":COMP_NAMES, "Score":sim_rtf}).iloc[1:6]
    st.table(df_rtf.style.format({"Score":"{:.2%}"}))

    # 3) Reviews Gensim-TFIDF
    toks_r = [doc.split() for doc in rev_grp]
    dct_r  = corpora.Dictionary(toks_r)
    corp_r = [dct_r.doc2bow(doc) for doc in toks_r]
    tfidf_r= models.TfidfModel(corp_r)
    idx_r  = similarities.MatrixSimilarity(tfidf_r[corp_r], num_features=len(dct_r))
    GEN_rv = np.vstack([ idx_r[tfidf_r[corp_r][i]] for i in range(len(corp_r)) ])
    sim_rgs= GEN_rv[idx]
    st.subheader("â€¢ Reviews Gensim-TFIDF")
    df_rgs = pd.DataFrame({"Company":COMP_NAMES, "Score":sim_rgs}).iloc[1:6]
    st.table(df_rgs.style.format({"Score":"{:.2%}"}))

    # 4) Reviews Word2Vec (IDF-weighted)
    idf_r   = tf_r.idf_
    w2v_r   = Word2Vec(toks_r, vector_size=100, window=5, min_count=2, epochs=10)
    def weighted_matrix(model, docs, idf, vocab):
        mat = []
        for doc in docs:
            vecs, ws = [], []
            for w in doc:
                if w in model.wv:
                    vecs.append(model.wv[w])
                    ws.append(idf[vocab.get(w,0)])
            mat.append(np.average(vecs, axis=0, weights=ws) if vecs else np.zeros(model.vector_size))
        return np.vstack(mat)
    mat_w2_r = weighted_matrix(w2v_r, toks_r, idf_r, tf_r.vocabulary_)
    sim_rw2  = cosine_similarity(mat_w2_r)[idx]
    st.subheader("â€¢ Reviews Word2Vec")
    df_rw2   = pd.DataFrame({"Company":COMP_NAMES, "Score":sim_rw2}).iloc[1:6]
    st.table(df_rw2.style.format({"Score":"{:.2%}"}))

    # 5) Reviews FastText (IDF-weighted)
    ft_r     = FastText(toks_r, vector_size=100, window=5, min_count=2, epochs=10)
    mat_ft_r = weighted_matrix(ft_r, toks_r, idf_r, tf_r.vocabulary_)
    sim_rft  = cosine_similarity(mat_ft_r)[idx]
    st.subheader("â€¢ Reviews FastText")
    df_rft   = pd.DataFrame({"Company":COMP_NAMES, "Score":sim_rft}).iloc[1:6]
    st.table(df_rft.style.format({"Score":"{:.2%}"}))
    st.stop()

# â”€â”€â”€ 7) Recommendation Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if menu == "ðŸ¤– Recommendation":
    st.header("4ï¸âƒ£ Recommendation Classification")

    # --- Chuáº©n bá»‹ dá»¯ liá»‡u ---
    tv = TfidfVectorizer(max_features=3000)
    Xr = tv.fit_transform(df_all["Clean_rev"].fillna(""))
    df_map_num = (
        df_map
        .set_index("id")
        .select_dtypes(include="number")
        .fillna(0)
    )
    num_mat = df_map_num.loc[df_all["id"]].values
    Xn      = sp.csr_matrix(num_mat)
    X       = sp.hstack([Xr, Xn])
    y       = df_all["Label"]

    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=0.2, random_state=42,
        stratify=y if y.nunique()>1 else None
    )
    X_res, y_res = SMOTE(random_state=42).fit_resample(X_tr, y_tr)

    # --- Scikit-Learn models (cached) ---
    @st.cache_data(show_spinner=False)
    def train_sk_models(_X_res, _y_res, _X_te, _y_te):
        clfs = {
            "LR":  LogisticRegression(class_weight="balanced", max_iter=500),
            "RF":  RandomForestClassifier(class_weight="balanced"),
            "SVM": SVC(probability=True, class_weight="balanced"),
            "XGB": XGBClassifier(use_label_encoder=False, eval_metric="logloss")
        }
        results = {}
        for name, clf in clfs.items():
            clf.fit(_X_res, _y_res)
            p    = clf.predict(_X_te)
            prob = clf.predict_proba(_X_te)[:,1]
            results[name] = {
                "pred": p,
                "prob": prob,
                "Acc":  accuracy_score(_y_te, p),
                "Prec": precision_score(_y_te, p, zero_division=0),
                "Rec":  recall_score(_y_te, p, zero_division=0),
                "F1":   f1_score(_y_te, p, zero_division=0),
                "AUC":  roc_auc_score(_y_te, prob)
            }
        return results

    sk_res = train_sk_models(X_res, y_res, X_te, y_te)

    # --- PySpark models (cached resource) ---
    @st.cache_resource(show_spinner=False)
    def train_spark_models(df_all):
        spark = SparkSession.builder.appName("itviec_cls").getOrCreate()
        sdf = spark.createDataFrame(
            df_all[["Clean_rev","Label"]]
            .rename(columns={"Clean_rev":"text","Label":"label"})
        )
        tok  = RegexTokenizer(inputCol="text",    outputCol="words",      pattern="\\W+")
        rem  = StopWordsRemover(inputCol="words",  outputCol="filtered")
        htf  = HashingTF(inputCol="filtered",      outputCol="rawFeatures", numFeatures=3000)
        idf  = IDF(inputCol="rawFeatures",         outputCol="features")
        pipe = Pipeline(stages=[tok, rem, htf, idf])
        data = pipe.fit(sdf).transform(sdf).select("features","label")
        tr, te = data.randomSplit([0.8,0.2], seed=42)

        ev_acc  = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
        ev_prec = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
        ev_rec  = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")
        ev_f1   = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
        ev_auc  = BinaryClassificationEvaluator(   labelCol="label", rawPredictionCol="probability", metricName="areaUnderROC")

        models_sp = {
            "SparkLR": SparkLR(labelCol="label", featuresCol="features"),
            "SparkDT": SparkDT(labelCol="label", featuresCol="features"),
            "SparkRF": SparkRF(labelCol="label", featuresCol="features")
        }
        sp_metrics = {}
        sp_preds   = {}
        for nm, est in models_sp.items():
            m   = est.fit(tr)
            prd = m.transform(te).withColumn("prediction", col("prediction").cast("double"))
            sp_metrics[nm] = {
                "Acc":  ev_acc.evaluate(prd),
                "Prec": ev_prec.evaluate(prd),
                "Rec":  ev_rec.evaluate(prd),
                "F1":   ev_f1.evaluate(prd),
                "AUC":  ev_auc.evaluate(prd)
            }
            sp_preds[nm] = prd.select("label","prediction","probability").toPandas()
        spark.stop()
        return sp_metrics, sp_preds

    sp_res, sp_preds = train_spark_models(df_all)

    # --- Hiá»ƒn thá»‹ 3 tab ---
    tab1, tab2, tab3 = st.tabs(["ðŸ”¹ Scikit-Learn","ðŸ”¸ PySpark","ðŸ¤– Interactive"])

    # Tab Scikit-Learn
    with tab1:
        st.subheader("Scikit-Learn Metrics")
        df_sk = pd.DataFrame({
            nm: {**{k:v for k,v in stats.items() if k in ["Acc","Prec","Rec","F1","AUC"]}}
            for nm,stats in sk_res.items()
        }).T
        st.dataframe(df_sk.style.format("{:.2%}"))

        # Confusion Matrix
        fig, axes = plt.subplots(2,2,figsize=(12,10)); axes=axes.flatten()
        for ax,(nm,stats) in zip(axes, sk_res.items()):
            ConfusionMatrixDisplay.from_predictions(y_te, stats["pred"], ax=ax)
            ax.set_title(nm)
        st.pyplot(fig)

        # ROC Curves
        fig2, ax2 = plt.subplots(figsize=(6,6))
        for nm,stats in sk_res.items():
            RocCurveDisplay.from_predictions(y_te, stats["prob"], name=nm, ax=ax2)
        st.pyplot(fig2)

    # Tab PySpark
    with tab2:
        st.subheader("PySpark Metrics")
        df_sp = pd.DataFrame(sp_res).T
        st.dataframe(df_sp.style.format("{:.2%}"))

        # Confusion Matrices
        cols = len(sp_preds)
        fig3, axes3 = plt.subplots(1, cols, figsize=(5*cols,4))
        if cols==1: axes3=[axes3]
        for ax,(nm,pdf) in zip(axes3, sp_preds.items()):
            ConfusionMatrixDisplay.from_predictions(
                pdf["label"].astype(int),
                pdf["prediction"].astype(int),
                ax=ax
            )
            ax.set_title(nm)
        st.pyplot(fig3)

        # ROC Curves
        fig4, ax4 = plt.subplots(figsize=(6,6))
        for nm,pdf in sp_preds.items():
            scores = pdf["probability"].apply(lambda v: v[1])
            RocCurveDisplay.from_predictions(pdf["label"].astype(int), scores, name=nm, ax=ax4)
        st.pyplot(fig4)

    # Tab Interactive
    with tab3:
        st.subheader("Interactive Prediction")
        best = max(sk_res.items(), key=lambda x: x[1]["Acc"])[0]
        st.markdown(f"**Best model:** {best}")
        clf = {
            "LR":  LogisticRegression(class_weight="balanced", max_iter=500),
            "RF":  RandomForestClassifier(class_weight="balanced"),
            "SVM": SVC(probability=True, class_weight="balanced"),
            "XGB": XGBClassifier(use_label_encoder=False, eval_metric="logloss")
        }[best]
        clf.fit(X_res, y_res)

        q = st.text_input("Nháº­p partial tÃªn cÃ´ng ty Ä‘á»ƒ dá»± Ä‘oÃ¡n Recommend?")
        if q:
            mask = df_all["Company Name"].str.contains(q, case=False, na=False)
            if not mask.any():
                st.warning("KhÃ´ng tÃ¬m tháº¥y cÃ´ng ty.")
            else:
                Xq   = X[mask.values]
                pr   = clf.predict_proba(Xq)[:,1]
                st.metric("Recommend rate", f"{(pr>0.5).mean():.2%}")

                df_detail = pd.DataFrame({
                    "Review": df_all.loc[mask, "Clean_rev"],
                    "Prob":   [f"{x:.2%}" for x in pr]
                })
                st.dataframe(df_detail, use_container_width=True)